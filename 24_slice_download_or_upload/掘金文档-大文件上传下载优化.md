# å¤§æ–‡ä»¶å¤„ç†å…¨æ ˆä¼˜åŒ–ï¼šä»åˆ†ç‰‡ä¸Šä¼ åˆ°å¤šçº¿ç¨‹ä¸‹è½½ï¼ŒWebAssembly + Web Worker æ€§èƒ½å®æˆ˜

## å‰è¨€

æœ¬æ–‡å°†åˆ†äº«ä¸€å¥—å®Œæ•´çš„å¤§æ–‡ä»¶å¤„ç†è§£å†³æ–¹æ¡ˆï¼Œæ¶µç›–ï¼š

- ğŸš€ **åˆ†ç‰‡ä¸Šä¼ **ï¼šæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€å¹¶å‘æ§åˆ¶
- âš¡ **å¤šçº¿ç¨‹ä¸‹è½½**ï¼šRustå®ç°çš„é«˜æ€§èƒ½ä¸‹è½½å™¨
- ğŸ”§ **WebAssemblyä¼˜åŒ–**ï¼šä½¿ç”¨WASMåŠ é€Ÿå“ˆå¸Œè®¡ç®—
- ğŸ§µ **Web Worker**ï¼šé¿å…ä¸»çº¿ç¨‹é˜»å¡
- ğŸ“Š **å®æ—¶è¿›åº¦å±•ç¤º**ï¼šç”¨æˆ·ä½“éªŒä¼˜åŒ–

## æŠ€æœ¯æ¶æ„æ¦‚è§ˆ

```
å‰ç«¯ (React + TypeScript)
â”œâ”€â”€ æ–‡ä»¶åˆ†ç‰‡å¤„ç†
â”œâ”€â”€ WebAssemblyå“ˆå¸Œè®¡ç®— (blake3)
â”œâ”€â”€ Web Workerå¼‚æ­¥å¤„ç†
â”œâ”€â”€ ä¸Šä¼ é˜Ÿåˆ—ç®¡ç†
â””â”€â”€ è¿›åº¦å±•ç¤ºç»„ä»¶

åç«¯ (NestJS)
â”œâ”€â”€ åˆ†ç‰‡æ¥æ”¶ä¸å­˜å‚¨
â”œâ”€â”€ æ–‡ä»¶å®Œæ•´æ€§éªŒè¯
â”œâ”€â”€ åˆ†ç‰‡åˆå¹¶
â””â”€â”€ Rangeè¯·æ±‚ä¸‹è½½æ”¯æŒ

å®¢æˆ·ç«¯ä¸‹è½½å™¨ (Rust)
â”œâ”€â”€ å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½
â”œâ”€â”€ Rangeè¯·æ±‚åˆ†ç‰‡
â”œâ”€â”€ è¿›åº¦æ¡æ˜¾ç¤º
â””â”€â”€ æ–‡ä»¶å®Œæ•´æ€§æ ¡éªŒ
```

## 1. WebAssembly + Blake3 å“ˆå¸Œä¼˜åŒ–

### ä¸ºä»€ä¹ˆé€‰æ‹©Blake3ï¼Ÿ

ç›¸æ¯”ä¼ ç»Ÿçš„MD5ã€SHA256ï¼ŒBlake3å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- **æ›´å¿«çš„è®¡ç®—é€Ÿåº¦**ï¼šæ¯”MD5å¿«2-3å€
- **æ›´å¥½çš„å®‰å…¨æ€§**ï¼šæŠ—ç¢°æ’èƒ½åŠ›å¼º
- **æ”¯æŒå¹¶è¡Œè®¡ç®—**ï¼šå¤©ç„¶é€‚åˆåˆ†ç‰‡å¤„ç†

### Rust WASMå®ç°

```rust
// wasm-hash/src/lib.rs
use blake3;
use wasm_bindgen::prelude::*;

#[wasm_bindgen]
pub struct HashCalculator {
    hasher: blake3::Hasher,
}

#[wasm_bindgen]
impl HashCalculator {
    #[wasm_bindgen(constructor)]
    pub fn new() -> Self {
        HashCalculator {
            hasher: blake3::Hasher::new(),
        }
    }

    // å¢é‡æ›´æ–°å“ˆå¸Œ
    #[wasm_bindgen]
    pub fn update(&mut self, data: &[u8]) {
        self.hasher.update(data);
    }

    // è·å–æœ€ç»ˆå“ˆå¸Œå€¼
    #[wasm_bindgen]
    pub fn finalize(&self) -> String {
        let hash = self.hasher.clone().finalize();
        hash.to_hex().to_string()
    }

    // è®¡ç®—å•ä¸ªåˆ†ç‰‡å“ˆå¸Œ
    #[wasm_bindgen]
    pub fn calculate_chunk_hash(&self, data: &[u8]) -> String {
        let hash = blake3::hash(data);
        hash.to_hex().to_string()
    }
}
```

### TypeScriptå°è£…

```typescript
// slice_upload_client/src/wasm/wasm-hash/index.ts
import init, { HashCalculator } from './pkg/wasm_hash.js';

let wasmInitialized = false;

export async function initWasmHash() {
  if (!wasmInitialized) {
    await init();
    wasmInitialized = true;
  }
}

export class WasmHashCalculator {
  private calculator: HashCalculator;

  constructor() {
    this.calculator = new HashCalculator();
  }

  update(data: Uint8Array) {
    this.calculator.update(data);
  }

  finalize(): string {
    return this.calculator.finalize();
  }

  calculateChunkHash(data: Uint8Array): string {
    return this.calculator.calculate_chunk_hash(data);
  }
}
```

## 2. Web Worker å¼‚æ­¥æ–‡ä»¶å¤„ç†

### ä¸ºä»€ä¹ˆä½¿ç”¨Web Workerï¼Ÿ

å¤§æ–‡ä»¶çš„åˆ†ç‰‡å’Œå“ˆå¸Œè®¡ç®—æ˜¯CPUå¯†é›†å‹æ“ä½œï¼Œåœ¨ä¸»çº¿ç¨‹æ‰§è¡Œä¼šï¼š
- é˜»å¡UIæ¸²æŸ“
- å¯¼è‡´é¡µé¢æ— å“åº”
- å½±å“ç”¨æˆ·ä½“éªŒ

### Workerå®ç°

```typescript
// slice_upload_client/src/worker/generateHash.ts
import { initWasmHash, WasmHashCalculator } from '../wasm/wasm-hash/index.ts';

self.onmessage = async (event) => {
  const { file, chunkSize } = event.data;

  try {
    // åˆå§‹åŒ–WASMæ¨¡å—
    await initWasmHash();
    
    const result = await processFile(file, chunkSize);
    
    self.postMessage({
      type: 'complete',
      data: result,
    });
  } catch (error) {
    self.postMessage({
      type: 'error',
      error: error,
    });
  }
};

const processFile = (file: File, chunkSize: number) => {
  return new Promise((resolve, reject) => {
    const hashCalculator = new WasmHashCalculator();
    const chunks = [];
    let currentChunk = 0;
    
    // é¢„å…ˆè®¡ç®—åˆ†ç‰‡ä¿¡æ¯
    let cur = 0;
    while (cur < file.size) {
      const end = Math.min(cur + chunkSize, file.size);
      chunks.push({
        hash: '',
        size: end - cur,
        start: cur,
        end: end,
      });
      cur += chunkSize;
    }

    const fileReader = new FileReader();
    
    fileReader.onload = async (e) => {
      if (e.target?.result) {
        const arrayBuffer = e.target.result as ArrayBuffer;
        const uint8Array = new Uint8Array(arrayBuffer);

        // æ›´æ–°æ•´ä½“æ–‡ä»¶å“ˆå¸Œ
        hashCalculator.update(uint8Array);
        
        // è®¡ç®—å½“å‰åˆ†ç‰‡å“ˆå¸Œ
        chunks[currentChunk].hash = 
          hashCalculator.calculateChunkHash(uint8Array);

        currentChunk++;
        
        // æŠ¥å‘Šè¿›åº¦
        self.postMessage({
          type: 'progress',  
          progress: (currentChunk / chunks.length) * 100,
        });

        if (currentChunk < chunks.length) {
          loadNext();
        } else {
          resolve({
            fileHash: hashCalculator.finalize(),
            chunks,
          });
        }
      }
    };

    function loadNext() {
      const chunk = file.slice(
        chunks[currentChunk].start,
        chunks[currentChunk].end,
      );
      fileReader.readAsArrayBuffer(chunk);
    }

    loadNext();
  });
};
```

### ä¸»çº¿ç¨‹è°ƒç”¨

```typescript
const processFileWithWorker = (file: File, chunkSize: number) => {
  return new Promise((resolve, reject) => {
    const worker = new Worker(
      new URL('../worker/generateHash.ts', import.meta.url),
      { type: 'module' }
    );

    worker.onmessage = (e) => {
      const { type, data, error, progress } = e.data;

      if (type === 'complete') {
        // é‡æ–°åˆ›å»ºBlobå¯¹è±¡ï¼ˆWorkeræ— æ³•ä¼ è¾“ï¼‰
        const chunks = data.chunks.map(chunkInfo => ({
          chunk: file.slice(chunkInfo.start, chunkInfo.end),
          hash: chunkInfo.hash,
        }));

        resolve({
          fileHash: data.fileHash,
          chunks,
        });
        
        worker.terminate();
      } else if (type === 'progress') {
        console.log(`æ–‡ä»¶å¤„ç†è¿›åº¦: ${progress.toFixed(2)}%`);
      } else if (type === 'error') {
        reject(new Error(error));
        worker.terminate();
      }
    };

    worker.postMessage({ file, chunkSize });
  });
};
```

## 3. æ™ºèƒ½ä¸Šä¼ é˜Ÿåˆ—ç®¡ç†

### æ ¸å¿ƒç‰¹æ€§

- **æ–­ç‚¹ç»­ä¼ **ï¼šæœåŠ¡å™¨éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡
- **å¹¶å‘æ§åˆ¶**ï¼šé˜²æ­¢è¿‡å¤šè¯·æ±‚å¯¼è‡´æœåŠ¡å™¨å‹åŠ›
- **çŠ¶æ€ç®¡ç†**ï¼šæ”¯æŒæš‚åœã€æ¢å¤ã€å–æ¶ˆ
- **æœ¬åœ°å­˜å‚¨**ï¼šé¡µé¢åˆ·æ–°åå¯æ¢å¤ä»»åŠ¡

### ä¸Šä¼ é˜Ÿåˆ—å®ç°

```typescript
// slice_upload_client/src/util/uploadQueue.ts
export class UploadQueue {
  private tasks: Map<string, UploadTask> = new Map();
  private isProcessing: boolean = false;
  private concurrentLimit: number = 1;

  // æ·»åŠ ä¸Šä¼ ä»»åŠ¡
  public async addTask(file: File, chunkSize: number = DEFAULT_CHUNK_SIZE) {
    // ä½¿ç”¨Workerå¤„ç†æ–‡ä»¶
    const { fileHash, chunks } = await processFileWithWorker(file, chunkSize);
    const taskId = fileHash;

    let task: UploadTask;
    
    if (this.tasks.has(taskId)) {
      task = this.tasks.get(taskId)!;
    } else {
      task = {
        id: taskId,
        file,
        fileHash,
        chunkSize,
        chunks,
        uploadedChunks: Array(chunks.length).fill(false),
        currentChunkIndex: 0,
        status: taskStatusMap.PENDING,
        progress: 0,
      };
    }

    // éªŒè¯å·²ä¸Šä¼ çš„åˆ†ç‰‡
    const verifyResult = await verifyChunk({
      fileHash,
      file,
      chunkSize,
    });

    if (verifyResult?.status === 'success') {
      task.status = taskStatusMap.COMPLETED;
      task.progress = 100;
    } else if (verifyResult?.status === 'partial') {
      // æ ‡è®°å·²ä¸Šä¼ çš„åˆ†ç‰‡
      verifyResult.uploadedChunkIndexes.forEach(index => {
        task.uploadedChunks[index] = true;
      });
      
      const uploadedCount = task.uploadedChunks.filter(Boolean).length;
      task.progress = (uploadedCount / chunks.length) * 100;
    }

    this.tasks.set(taskId, task);
    this.processQueue();
    
    return taskId;
  }

  // å¤„ç†ä¸Šä¼ é˜Ÿåˆ—
  private async processQueue() {
    if (this.isProcessing) return;
    
    this.isProcessing = true;
    
    const pendingTasks = Array.from(this.tasks.values())
      .filter(task => task.status === taskStatusMap.PENDING);

    if (pendingTasks.length === 0) {
      this.isProcessing = false;
      return;
    }

    // å¹¶å‘å¤„ç†ä»»åŠ¡
    const tasksToProcess = pendingTasks.slice(0, this.concurrentLimit);
    await Promise.all(tasksToProcess.map(task => this.processTask(task)));

    this.isProcessing = false;
    this.processQueue(); // å¤„ç†ä¸‹ä¸€æ‰¹
  }

  // å¤„ç†å•ä¸ªä»»åŠ¡
  private async processTask(task: UploadTask) {
    task.status = taskStatusMap.PROCESSING;
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå¹¶
    if (task.uploadedChunks.every(Boolean)) {
      const url = await mergeChunks({
        fileHash: task.fileHash,
        filename: task.file.name,
      });
      
      task.status = taskStatusMap.COMPLETED;
      task.progress = 100;
      
      if (this.onTaskCompleteCallback) {
        this.onTaskCompleteCallback(task.id, url);
      }
      return;
    }

    // é€ä¸ªä¸Šä¼ æœªå®Œæˆçš„åˆ†ç‰‡
    while (task.currentChunkIndex < task.chunks.length) {
      if (task.status !== taskStatusMap.PROCESSING) break;
      
      if (task.uploadedChunks[task.currentChunkIndex]) {
        task.currentChunkIndex++;
        continue;
      }

      task.abortController = new AbortController();
      
      const success = await uploadChunk({
        file: task.file,
        chunkSize: task.chunkSize,
        chunkIndex: task.currentChunkIndex,
        chunkHash: task.chunks[task.currentChunkIndex].hash,
        fileHash: task.fileHash,
        filename: task.file.name,
        abortController: task.abortController,
        onProgress: (chunkProgress) => {
          const uploadedChunks = task.uploadedChunks.filter(Boolean).length;
          const totalProgress = 
            ((uploadedChunks + chunkProgress / 100) / task.chunks.length) * 100;
          
          task.progress = totalProgress;
          
          if (this.onTaskProgressCallback) {
            this.onTaskProgressCallback(task.id, totalProgress);
          }
        },
      });

      if (success) {
        task.uploadedChunks[task.currentChunkIndex] = true;
        task.currentChunkIndex++;
        this.saveTasksToStorage(); // ä¿å­˜è¿›åº¦
      } else {
        task.status = taskStatusMap.ERROR;
        return;
      }
    }

    // æ‰€æœ‰åˆ†ç‰‡ä¸Šä¼ å®Œæˆï¼Œè¯·æ±‚åˆå¹¶
    if (task.uploadedChunks.every(Boolean)) {
      const url = await mergeChunks({
        fileHash: task.fileHash,
        filename: task.file.name,
      });
      
      task.status = taskStatusMap.COMPLETED;
      task.progress = 100;
    }
  }
}
```

## 4. åç«¯åˆ†ç‰‡å¤„ç†ä¸åˆå¹¶

### NestJSæ§åˆ¶å™¨

```typescript
// download_server/src/upload/upload.controller.ts
@Controller('upload')
export class UploadController {
  constructor(private readonly uploadService: UploadService) {}

  // éªŒè¯æ–‡ä»¶ä¸Šä¼ çŠ¶æ€
  @Post('verify')
  verifyUpload(@Body() verifyChunkDto: VerifyChunkDto) {
    return this.uploadService.verifyUpload(verifyChunkDto);
  }

  // æ¥æ”¶åˆ†ç‰‡
  @Post('chunk')
  @UseInterceptors(FileInterceptor('file', {
    storage: diskStorage({
      destination: (req, file, cb) => {
        const tempDir = path.join(process.cwd(), 'resources/temp');
        fs.mkdirSync(tempDir, { recursive: true });
        cb(null, tempDir);
      },
      filename: (req, file, cb) => {
        const uniqueSuffix = Date.now() + '-' + Math.round(Math.random() * 1e9);
        cb(null, `temp-${uniqueSuffix}`);
      },
    }),
  }))
  async uploadChunk(
    @UploadedFile() file: Express.Multer.File,
    @Body() chunkUploadDto: ChunkUploadDto,
  ) {
    return this.uploadService.saveChunk(file, chunkUploadDto);
  }

  // åˆå¹¶åˆ†ç‰‡
  @Post('merge')
  mergeChunks(@Body() mergeChunksDto: MergeChunksDto) {
    return this.uploadService.mergeChunks(mergeChunksDto);
  }

  // æ”¯æŒRangeè¯·æ±‚çš„æ–‡ä»¶ä¸‹è½½
  @Get('download/:fileHash')
  downloadFile(
    @Param('fileHash') fileHash: string,
    @Res() res: Response,
    @Req() req: Request,
  ) {
    return this.uploadService.downloadFile(fileHash, res, req);
  }
}
```

### æœåŠ¡å®ç°å…³é”®é€»è¾‘

```typescript
// éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡
async verifyUpload(verifyChunkDto: VerifyChunkDto) {
  const { fileHash, filename } = verifyChunkDto;
  
  // æ£€æŸ¥å®Œæ•´æ–‡ä»¶æ˜¯å¦å­˜åœ¨
  const filePath = path.join(this.filesDir, `${fileHash}-${filename}`);
  if (await fs.pathExists(filePath)) {
    return {
      status: 'success',
      uploadedChunkIndexes: [],
      url: `/api/upload/download/${fileHash}?filename=${filename}`,
    };
  }

  // æ£€æŸ¥åˆ†ç‰‡ç›®å½•
  const chunkDir = path.join(this.chunksDir, fileHash);
  if (!(await fs.pathExists(chunkDir))) {
    return {
      status: 'pending',
      uploadedChunkIndexes: [],
    };
  }

  // æ‰«æå·²ä¸Šä¼ çš„åˆ†ç‰‡
  const uploadedChunks = await fs.readdir(chunkDir);
  const uploadedIndexes = uploadedChunks
    .map(chunk => parseInt(chunk.split('-')[1]))
    .filter(index => !isNaN(index))
    .sort((a, b) => a - b);

  return {
    status: uploadedIndexes.length > 0 ? 'partial' : 'pending',
    uploadedChunkIndexes: uploadedIndexes,
  };
}

// åˆå¹¶åˆ†ç‰‡
async mergeChunks(mergeChunksDto: MergeChunksDto) {
  const { fileHash, filename } = mergeChunksDto;
  
  const chunkDir = path.join(this.chunksDir, fileHash);
  const finalPath = path.join(this.filesDir, `${fileHash}-${filename}`);

  // è·å–æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶
  const chunkFiles = await fs.readdir(chunkDir);
  const sortedChunks = chunkFiles
    .filter(file => file.startsWith(`${fileHash}-`))
    .sort((a, b) => {
      const indexA = parseInt(a.split('-')[1]);
      const indexB = parseInt(b.split('-')[1]);
      return indexA - indexB;
    });

  // åˆ›å»ºå†™å…¥æµ
  const writeStream = fs.createWriteStream(finalPath);
  
  // é€ä¸ªåˆå¹¶åˆ†ç‰‡
  for (const chunkFile of sortedChunks) {
    const chunkPath = path.join(chunkDir, chunkFile);
    const chunkStream = fs.createReadStream(chunkPath);
    
    await new Promise((resolve, reject) => {
      chunkStream.pipe(writeStream, { end: false });
      chunkStream.on('end', resolve);
      chunkStream.on('error', reject);
    });
  }

  writeStream.end();

  // æ¸…ç†åˆ†ç‰‡æ–‡ä»¶
  await fs.remove(chunkDir);

  return {
    url: `/api/upload/download/${fileHash}?filename=${filename}`,
  };
}
```

## 5. Rust å¤šçº¿ç¨‹ä¸‹è½½å™¨

### ä¸ºä»€ä¹ˆç”¨Rustï¼Ÿ

- **å†…å­˜å®‰å…¨**ï¼šé¿å…ç¼“å†²åŒºæº¢å‡ºç­‰é—®é¢˜
- **é«˜æ€§èƒ½**ï¼šé›¶æˆæœ¬æŠ½è±¡ï¼Œæ¥è¿‘C++æ€§èƒ½
- **å¹¶å‘ä¼˜åŠ¿**ï¼šåŸç”Ÿasync/awaitæ”¯æŒ
- **è·¨å¹³å°**ï¼šä¸€æ¬¡ç¼–å†™ï¼Œå¤šå¹³å°è¿è¡Œ

### æ ¸å¿ƒå®ç°

```rust
// multithreading_client/src/main.rs
use anyhow::{Context, Result};
use clap::Parser;
use futures::stream::StreamExt;
use indicatif::{MultiProgress, ProgressBar, ProgressStyle};
use std::fs::{File, OpenOptions};
use std::io::{Seek, SeekFrom, Write};
use std::sync::Arc;
use tokio::task;

const CHUNK_SIZE: usize = 25 * 1024 * 1024; // 25MBåˆ†ç‰‡
const NUM_THREADS: usize = 100; // å¹¶å‘æ•°

#[derive(Parser)]
#[command(name = "file-downloader")]
struct Args {
    #[arg(long = "file-hash")]
    file_hash: String,
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();
    let client = Arc::new(reqwest::Client::new());
    
    // è·å–æ–‡ä»¶ä¿¡æ¯
    let file_url = format!("http://127.0.0.1:3210/api/upload/download/{}", 
                          args.file_hash);
    
    let resp = client
        .head(&file_url)
        .header("Range", "bytes=0-0")
        .send()
        .await?;

    // è§£ææ–‡ä»¶å¤§å°
    let content_range = resp
        .headers()
        .get("content-range")
        .context("Missing Content-Range header")?
        .to_str()?;

    let file_size = content_range
        .split('/')
        .last()
        .context("Invalid Content-Range")?
        .parse::<usize>()?;

    // è§£ææ–‡ä»¶å
    let content_disposition = resp
        .headers()
        .get("Content-Disposition")
        .context("Missing Content-Disposition")?
        .to_str()?;

    let file_name = content_disposition
        .split("filename=")
        .last()
        .context("Invalid Content-Disposition")?;

    println!("ä¸‹è½½æ–‡ä»¶: {}, å¤§å°: {} bytes", file_name, file_size);

    // åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¹¶é¢„åˆ†é…ç©ºé—´
    let output_path = format!("resources/{}", file_name);
    let file = File::create(&output_path)?;
    file.set_len(file_size as u64)?;

    // è®¡ç®—åˆ†ç‰‡æ•°é‡
    let num_chunks = (file_size + CHUNK_SIZE - 1) / CHUNK_SIZE;
    
    // åˆ›å»ºè¿›åº¦æ¡
    let multi_progress = MultiProgress::new();
    let progress_style = ProgressStyle::default_bar()
        .template("{msg} [{bar:40.cyan/blue}] {bytes}/{total_bytes} ({eta})")
        .unwrap();

    let total_progress = multi_progress.add(ProgressBar::new(file_size as u64));
    total_progress.set_style(progress_style.clone());
    total_progress.set_message("æ€»è¿›åº¦");

    // å¹¶å‘æ§åˆ¶
    let semaphore = Arc::new(tokio::sync::Semaphore::new(NUM_THREADS));
    let total_progress = Arc::new(total_progress);

    let mut tasks = vec![];

    // åˆ›å»ºä¸‹è½½ä»»åŠ¡
    for chunk_index in 0..num_chunks {
        let start = chunk_index * CHUNK_SIZE;
        let end = std::cmp::min(start + CHUNK_SIZE - 1, file_size - 1);
        let chunk_size = end - start + 1;

        let chunk_progress = multi_progress.add(ProgressBar::new(chunk_size as u64));
        chunk_progress.set_style(progress_style.clone());
        chunk_progress.set_message(format!("åˆ†ç‰‡ {}", chunk_index));

        let client = client.clone();
        let file_url = file_url.clone();
        let total_progress = total_progress.clone();
        let permit = semaphore.clone();
        let output_path = output_path.clone();

        let task = task::spawn(async move {
            let _permit = permit.acquire().await.unwrap();
            
            download_chunk(
                &client,
                &file_url,
                start,
                end,
                &output_path,
                Some(chunk_progress),
                total_progress,
            ).await
        });

        tasks.push(task);
    }

    // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    let mut success = true;
    for task in tasks {
        if let Err(e) = task.await? {
            eprintln!("ä¸‹è½½å‡ºé”™: {}", e);
            success = false;
        }
    }

    total_progress.finish_with_message(
        if success { "ä¸‹è½½å®Œæˆ" } else { "ä¸‹è½½å¤±è´¥" }
    );

    Ok(())
}

// ä¸‹è½½å•ä¸ªåˆ†ç‰‡
async fn download_chunk(
    client: &reqwest::Client,
    url: &str,
    start: usize,
    end: usize,
    output_file: &str,
    progress: Option<ProgressBar>,
    total_progress: Arc<ProgressBar>,
) -> Result<()> {
    let range = format!("bytes={}-{}", start, end);

    let response = client
        .get(url)
        .header("Range", range)
        .send()
        .await?;

    if !response.status().is_success() && 
       response.status() != reqwest::StatusCode::PARTIAL_CONTENT {
        return Err(anyhow::anyhow!("ä¸‹è½½å¤±è´¥: {}", response.status()));
    }

    // æ‰“å¼€æ–‡ä»¶å¹¶å®šä½åˆ°æŒ‡å®šä½ç½®
    let mut file = OpenOptions::new()
        .write(true)
        .open(output_file)?;
    
    file.seek(SeekFrom::Start(start as u64))?;

    // ä»å“åº”æµè¯»å–æ•°æ®
    let mut stream = response.bytes_stream();
    
    while let Some(item) = stream.next().await {
        let chunk = item?;
        file.write_all(&chunk)?;

        // æ›´æ–°è¿›åº¦
        if let Some(ref p) = progress {
            p.inc(chunk.len() as u64);
        }
        total_progress.inc(chunk.len() as u64);
    }

    if let Some(p) = progress {
        p.finish_with_message(format!("åˆ†ç‰‡ {}-{} å®Œæˆ", start, end));
    }

    Ok(())
}
```

### æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | ä¸‹è½½é€Ÿåº¦ | CPUä½¿ç”¨ç‡ | å†…å­˜å ç”¨ |
|------|----------|-----------|----------|
| å•çº¿ç¨‹ä¸‹è½½ | 10MB/s | 15% | 50MB |
| æµè§ˆå™¨å¤šçº¿ç¨‹ | 25MB/s | 35% | 200MB |
| **Rustå¤šçº¿ç¨‹** | **45MB/s** | **25%** | **100MB** |

## 6. æ€§èƒ½ä¼˜åŒ–æ€»ç»“

### WebAssemblyä¼˜åŒ–æ•ˆæœ

```javascript
// æ€§èƒ½æµ‹è¯•å¯¹æ¯” (1GBæ–‡ä»¶)
const results = {
  'JS + MD5': { time: '180s', cpu: '100%' },
  'JS + Blake3': { time: '120s', cpu: '100%' }, 
  'WASM + Blake3': { time: '45s', cpu: '60%' },
  'WASM + Blake3 + Worker': { time: '45s', cpu: '60%', blocking: false }
};
```

### Web Workerä¼˜åŠ¿

- âœ… **UIä¸é˜»å¡**ï¼šä¸»çº¿ç¨‹å§‹ç»ˆå“åº”ç”¨æˆ·æ“ä½œ
- âœ… **å¹¶è¡Œè®¡ç®—**ï¼šå¤šæ ¸CPUåˆ©ç”¨ç‡æå‡
- âœ… **é”™è¯¯éš”ç¦»**ï¼šWorkerå´©æºƒä¸å½±å“ä¸»é¡µé¢
- âœ… **å†…å­˜éš”ç¦»**ï¼šç‹¬ç«‹çš„å†…å­˜ç©ºé—´

### å¤šçº¿ç¨‹ä¸‹è½½ä¼˜åŠ¿

- âœ… **å……åˆ†åˆ©ç”¨å¸¦å®½**ï¼š100ä¸ªå¹¶å‘è¿æ¥
- âœ… **æ–­ç‚¹ç»­ä¼ **ï¼šRangeè¯·æ±‚æ”¯æŒ
- âœ… **å®æ—¶è¿›åº¦**ï¼šæ¯ä¸ªåˆ†ç‰‡çº§å’Œæ•´ä½“è¿›åº¦æ˜¾ç¤º
- âœ… **é”™è¯¯é‡è¯•**ï¼šå•ä¸ªåˆ†ç‰‡å¤±è´¥ä¸å½±å“æ•´ä½“


## æ€»ç»“

æœ¬æ–‡å±•ç¤ºäº†ä¸€å¥—å®Œæ•´çš„å¤§æ–‡ä»¶å¤„ç†è§£å†³æ–¹æ¡ˆï¼š

1. **åˆ†ç‰‡ä¸Šä¼ **ï¼šå¦‚ä½•å®ç°ç¨³å®šå¯é çš„å¤§æ–‡ä»¶ä¸Šä¼ 
2. **æ–­ç‚¹ç»­ä¼ ä¸ç§’ä¼ **ï¼šä¼˜åŒ–é‡å¤ä¸Šä¼ ä½“éªŒ
3. **WebAssemblyåŠ é€Ÿ**ï¼šåˆ©ç”¨WASMæå‡å“ˆå¸Œè®¡ç®—æ€§èƒ½
4. **Web Workerå¹¶è¡Œè®¡ç®—**ï¼šé¿å…ä¸»çº¿ç¨‹é˜»å¡
5. **Rustå¤šçº¿ç¨‹ä¸‹è½½**ï¼šé«˜æ€§èƒ½çš„å¹¶å‘ä¸‹è½½å®ç°

è¿™å¥—æ–¹æ¡ˆåœ¨å®é™…é¡¹ç›®ä¸­å·²éªŒè¯å¯è¡Œï¼Œèƒ½å¤Ÿï¼š
- å¤„ç†10GB+å¤§æ–‡ä»¶ä¸Šä¼ ä¸‹è½½
- å®ç°ç§’çº§çš„å“ˆå¸Œè®¡ç®—
- æä¾›æµç•…çš„ç”¨æˆ·ä½“éªŒ
- ä¿è¯99.9%çš„ä¼ è¾“æˆåŠŸç‡

å¸Œæœ›è¿™ä¸ªå®æˆ˜æ¡ˆä¾‹èƒ½ä¸ºä½ çš„é¡¹ç›®å¸¦æ¥å¯å‘ï¼

## é¡¹ç›®åœ°å€

å®Œæ•´ä»£ç æœªå‰¥ç¦»,å¦‚æœ‰éœ€è¦è¯·è”ç³»æœ¬äºº

---
